\SetPicSubDir{ch-Comparison}
\SetExpSubDir{ch-Comparison}

\chapter{Comparison Machine Learning Models}
\label{ch:comparison}
\vspace{2em}

\section{$k$-means Clustering}

$k$-means clustering partitions a given set of samples $x_1, \cdots, x_n$ into
$k$ disjoint sets $C$, each described by the mean $\mu_j$ of the samples in the
cluster;
it produces a partition that minimizes the within-cluster sum-of-squares (WCSS):

\[
    \sum_{i=1}^{n} \underset{\mu_j \in C}{\min}(||x_i - \mu_j||^2)
\]

$k$-means clustering has the advantage of being a general purpose method
requiring little hyperparameter tuning. It also runs quickly in practice.
The main drawback of $k$-means is that it assumes the clusters to be of similar size.
Our ground truth partition by party isn't an extreme case of dissimilar sized
clusters; it definitely has some variability\footnote{party size statistics:
max=34, min=6, median=11, mean=14.7, stdev=9.6}.

Since $k$-means produces a partition of $k$ clusters by minimizing within-cluster
sum-of-squares (WCSS), we need to define the distance between points and choose
an appropriate $k$ value.

Let each politician corresponds to a point; each bill is a feature.
We assign a ``for'' vote on a bill a value of 1, ``against'' vote a value of -1;
any other vote value, including missing and ``abstained'', a value of 0.
This simple vote value encoding captures the idea that ``for'' and ``against''
votes are far from each other; anything else is neutral and has equal distance
from ``for'' and ``against''.

We run $k$-means clustering for $k$ in range $[2, 50]$, and determine the best
$k=10$ by plotting the number of clusters against the corresponding partition's
total WCSS and picking the point where there is a ``bend'' in the plot
--- this is known as the elbow method for $k$ selection.
The elbow in the plot represents a point where adding one more cluster does not
bring much improvement to WCSS anymore.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{k_means_wcss}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Average WCSS of Knesset partitions from $k$-means clusterings}
  \label{Comparison:fig:k_means_wcss}
\end{figure}

One might argue that the elbow method is not very methodological as it relies
solely on visual assessment.
Average silhouette is an alternative method which provides a quantitative
definitive selection of $k$.
The \textit{silhouette score} $s$ for a given sample is
$s = \frac{b - a}{\max(a, b)}$, where $a$ is the mean distance between this
sample and all other points in the same cluster; $b$ is the mean distance
between this sample and all other points in the next nearest cluster.
Observe that $-1 \leq s \leq 1$; when a clustering is good, we expect $a << b$
meaning $s$ would be close to 1.
The silhouette method looks at the average silhouette score of each partition
generated by various k values, and picks the one with the highest average
silhouette score.
On our Knesset dataset the average silhouette peaks at $k=2$ and decreases
significantly thereafter.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{k_means_silhouette}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Average silhouette scores of Knesset partitions from $k$-means clusterings}
  \label{Comparison:fig:k_means_silhouette}
\end{figure}

\section{Stochastic Block Models}
