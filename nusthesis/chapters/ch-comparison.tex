\SetPicSubDir{ch-Comparison}
\SetExpSubDir{ch-Comparison}

\chapter{Machine Learning Models}
\label{ch:comparison}
Hedonic coalition formation is, at its essence, a method of partitioning a set
of points (players) into groups.
Partitioning a given set of points is a well studied field in machine learning
known as {\em cluster analysis} or clustering;
when the set of objects can be modeled as a network, people also refer to the
partitioning task as {\em community detection}.

In this chapter, we explore two classes of machine learning models commonly used
in clustering and community detection.
We apply these models to the Knesset dataset, producing Knesset member partitions.
These techniques serve as a baseline;
in \autoref{ch:analysis} we compare the resulting partitions to those of our game
theoretical models described in \autoref{ch:hedonic}.


\section{$k$-Means Clustering}
\label{sec:k_means_clustering}

$k$-means clustering\cite{Tan:2018:IDM:3208440} divides a given set of samples
$x_1, \cdots, x_n$\footnote{Each sample $x_i$ is a vector of feature values.
A feature is an ``individual measurable property or characteristic of a
phenomenon being observed'' \cite{Bishop:2006:PRM:1162264}} into $k$ disjoint
sets $C$ (equivalently, $C$ is a partition of samples), each described by the
mean $\mu_j$ of the samples in the cluster;
it produces a partition minimizing the {\em within-cluster sum-of-squares} (WCSS):

\[
    \sum_{i=1}^{n} \underset{\mu_j \in C}{\min}(||x_i - \mu_j||^2)
\]

$k$-means clustering has the advantage of being a general-purpose method
requiring little hyperparameter tuning.
It also runs quickly in practice.
The main drawback of $k$-means is that it assumes the clusters to be of similar
size.
Our ground truth partition along party lines does not exhibit extremely
dissimilar sized clusters, but it definitely has some variability (Party size
statistics: max=34, min=6, median=11, mean=14.7, stdev=9.6).

Since $k$-means produces a partition of $k$ clusters by minimizing within-cluster
sum-of-squares (WCSS), we need to define the distance between points and choose
an appropriate $k$ value.

We propose the following clustering method: we let each politician correspond to
a point, with each bill acting as a feature.
We assign a ``for'' vote on a bill a value of 1, ``against'' vote a value of -1;
any other vote value, including missing and ``abstained'', a value of 0.
This simple vote value encoding captures the idea that ``for'' and ``against''
votes are far from each other; anything else is neutral and is equidistant
to ``for'' and ``against''.

We run $k$-means clustering for $k$ in the range $[2, 50]$, and determine the
best value ($k=10$) by plotting the number of clusters against the corresponding
partition's total WCSS.
We then pick the point where there is a ``bend'' in the plot
--- this is known as the elbow method for $k$ selection
\cite{Thorndike53whobelongs}.
The elbow in the plot represents a point where adding one more cluster does not significantly improve WCSS.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{k_means_wcss}}
  \caption{Average WCSS of Knesset partitions from $k$-means clusterings}
  \label{Comparison:fig:k_means_wcss}
\end{figure}

One might argue that the elbow method is not very methodological as it relies
solely on visual assessment.
Average silhouette provides a quantitative alternative
\cite{Rousseeuw:1987:SGA:38768.38772}.
The \textit{silhouette score} $s$ for a given sample is
$s = \frac{b - a}{\max(a, b)}$, where $a$ is the mean distance between this
sample and all other points in the same cluster; $b$ is the mean distance
between this sample and all other points in the next nearest cluster.
Observe that $-1 \leq s \leq 1$; when a clustering is good, we expect $a << b$
meaning $s$ would be close to 1.
The silhouette method looks at the average silhouette score of each partition
generated by various $k$ values, and picks the one with the highest average
silhouette score.
On our Knesset dataset the average silhouette peaks at $k=2$ and decreases
significantly thereafter.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{k_means_silhouette}}
  \caption{Average silhouette scores of Knesset partitions from $k$-means clusterings}
  \label{Comparison:fig:k_means_silhouette}
\end{figure}

We present results from both $k=2$ and $k=10$ in \autoref{ch:analysis}.

\section{Stochastic Block Model}
\label{sec:stochastic_block_model}

The stochastic block model (SBM) \cite{HOLLAND1983109} commonly
serves as a benchmark model in community detection.
This approach models dataset as a graph; for our Knesset dataset, parliament
members make the nodes of this graph and how frequently each pair of members
voted together/differently form the edge weights\footnote{Since we need edge
weights in the graph model, we adopt the \textit{Nonparametric weighted
stochastic block model} implementation by \citenameyear{Peixoto}}.
The stochastic block model assumes nodes that belong to the same group have
the same probability of being connected with other nodes of the network.
Using Bayesian inference, it finds a partition that maximizes the likelihood of
the observed network.
It is equivalent to the \textit{minimum description length method} where given
the same explanatory power, the simplest model is selected.
This gives the SBM approach some tolerance for stochastic noise.

We construct the first SBM with only positive edge weights --- each edge weight
represents the number of times a pair of politicians voted together, either
``for'' or ``against'' a bill.
We model the edge weights using the geometric distribution given the weights are
non-negative and discrete values.
We plot the normalized frequencies of edge weights in
\autoref{Comparison:fig:sbm_edge_weight_hist}.
Observe that the general shape of the plot follows that of the probability mass function for a geometric distribution.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{sbm_edge_weight_hist}}
  \caption{The Knesset under SBM: normalized histogram of positive edge weights}
  \label{Comparison:fig:sbm_edge_weight_hist}
\end{figure}

Our second SBM accounts for instances when a pair of politicians' votes
disagree --- edge weight is the difference between the number of times their votes
agree and the number of times their votes disagree.
Since the edge weights can be negative, we model the weights using the normal
distribution.
See \autoref{Comparison:fig:sbm_edge_weight_neg_hist} for the normalized
frequencies of potentially negative edge weights.
The overall shape of the plot is single-peaked like the bell shape of a normal
distribution.
However, upon close inspection, we observe that the distribution
may be better described by a mixture of three normal distributions, with one
centered in the negative range, one around zero, and one in the positive range;
unfortunately such a model is beyond the complexity we are willing to explore for
a baseline model.
We discuss the implication of using the normal distribution to approximate the
edge weights in \autoref{sec:quantitative_analysis}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{sbm_edge_weight_neg_hist}}
  \caption{The Knesset under SBM: normalized histogram of possible negative edge weights}
  \label{Comparison:fig:sbm_edge_weight_neg_hist}
\end{figure}

Given that we are analysing an empirical network, there could be more than one
fit of the SBM with similar likelihoods of generating the observed network.
Therefore instead of finding the maximum, we sample partitions from the
posterior distribution, and take the average weighted by posterior probabilities
across different model fits (also known as model averaging).
We use the SBM implemented in the \texttt{graph-tool} library, which supports
model averaging using an efficient Markov chain Monte Carlo (MCMC) algorithm
\cite{PhysRevE.89.012804}, further optimizing the SBM models.
