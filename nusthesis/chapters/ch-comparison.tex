\SetPicSubDir{ch-Comparison}
\SetExpSubDir{ch-Comparison}

\chapter{Machine Learning Models}
\label{ch:comparison}
\vspace{2em}

Partitioning a given set of objects is a well studied field in machine learning
known as cluster analysis or clustering;
when the set of objects can be modeled as a network, people also refer to the
task of partitioning as community detection.

In this chapter, we explore two classes of benchmark machine learning models
commonly used in clustering and community detection.
We apply these two class of models on the Knesset dataset to produce partitions
of parliament members.
In Chapter~\ref{ch:analysis} we compare the resulting partitions to those of our
game theoretical models described in Chapter~\ref{ch:hedonic}.


\section{$k$-means Clustering}
\label{sec:k_means_clustering}

$k$-means clustering partitions a given set of samples $x_1, \cdots, x_n$ into
$k$ disjoint sets $C$, each described by the mean $\mu_j$ of the samples in the
cluster;
it produces a partition that minimizes the within-cluster sum-of-squares (WCSS):

\[
    \sum_{i=1}^{n} \underset{\mu_j \in C}{\min}(||x_i - \mu_j||^2)
\]

$k$-means clustering has the advantage of being a general purpose method
requiring little hyperparameter tuning. It also runs quickly in practice.
The main drawback of $k$-means is that it assumes the clusters to be of similar size.
Our ground truth partition by party isn't an extreme case of dissimilar sized
clusters; it definitely has some variability\footnote{party size statistics:
max=34, min=6, median=11, mean=14.7, stdev=9.6}.

Since $k$-means produces a partition of $k$ clusters by minimizing within-cluster
sum-of-squares (WCSS), we need to define the distance between points and choose
an appropriate $k$ value.

Let each politician corresponds to a point; each bill is a feature.
We assign a ``for'' vote on a bill a value of 1, ``against'' vote a value of -1;
any other vote value, including missing and ``abstained'', a value of 0.
This simple vote value encoding captures the idea that ``for'' and ``against''
votes are far from each other; anything else is neutral and has equal distance
from ``for'' and ``against''.

We run $k$-means clustering for $k$ in range $[2, 50]$, and determine the best
$k=10$ by plotting the number of clusters against the corresponding partition's
total WCSS and picking the point where there is a ``bend'' in the plot
--- this is known as the elbow method for $k$ selection.
The elbow in the plot represents a point where adding one more cluster does not
bring much improvement to WCSS anymore.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{k_means_wcss}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Average WCSS of Knesset partitions from $k$-means clusterings}
  \label{Comparison:fig:k_means_wcss}
\end{figure}

One might argue that the elbow method is not very methodological as it relies
solely on visual assessment.
Average silhouette is an alternative method which provides a quantitative
definitive selection of $k$.
The \textit{silhouette score} $s$ for a given sample is
$s = \frac{b - a}{\max(a, b)}$, where $a$ is the mean distance between this
sample and all other points in the same cluster; $b$ is the mean distance
between this sample and all other points in the next nearest cluster.
Observe that $-1 \leq s \leq 1$; when a clustering is good, we expect $a << b$
meaning $s$ would be close to 1.
The silhouette method looks at the average silhouette score of each partition
generated by various k values, and picks the one with the highest average
silhouette score.
On our Knesset dataset the average silhouette peaks at $k=2$ and decreases
significantly thereafter.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{k_means_silhouette}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Average silhouette scores of Knesset partitions from $k$-means clusterings}
  \label{Comparison:fig:k_means_silhouette}
\end{figure}

We present results from both $k=2$ and $k=10$ in Chapter~\ref{ch:analysis}.

\section{Stochastic Block Model}
\label{sec:stochastic_block_model}

Stochastic block model (SBM) commonly serves as a benchmark model in community
detection.
This approach models dataset as a graph; for our Knesset dataset, parliament
members make the nodes of this graph and how frequently each pair of members
voted together/differently form the edge weights.
The stochastic block model assumes nodes that belong to the same group have
the same probability of being connected with other nodes of the network.
Using Bayesian inference, it finds a partition that maximizes the likelihood of
observed network.
It is equivalent to the \textit{minimum description length method} where given the
same explanatory power, the simplest model is selected.
This gives the SBM approach some tolerance for stochastic noise.

We construct the first SBM with only positive edge weights --- each edge weight
represents the number of times a pair of politicians voted together, either
``for'' or ``against'' a bill.
We model the edge weights using the geometric distribution given the weights are
non-negative and discrete values.
We plot the normalized frequencies of edge weights in
Figure~\ref{Comparison:fig:sbm_edge_weight_hist}.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{sbm_edge_weight_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{The Knesset under SBM: normalized histogram of positive edge weights}
  \label{Comparison:fig:sbm_edge_weight_hist}
\end{figure}

Our second SBM accounts for instances when a pair of politicians' votes
disagreed --- edge weight is the difference of the number of times their votes
agreed and the number of times their votes disagreed.
Since the edge weights can be negative, we model the weights using the normal
distribution.
See Figure~\ref{Comparison:fig:sbm_edge_weight_neg_hist} for the normalized
frequencies of potentially negative edge weights.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{\Pic{png}{sbm_edge_weight_neg_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{The Knesset under SBM: normalized histogram of possible negative edge weights}
  \label{Comparison:fig:sbm_edge_weight_neg_hist}
\end{figure}

Given that we are analysing an empirical network, there could be more than one
fit of the SBM with similar likelihoods of generating the observed network.
Therefore instead of finding the maximum, we sample partitions from the
posterior distribution, and take the average weighted by posterior probabilities
across different model fits (aka.\ model averaging).
We use the SBM implemented in the graph-tool library, which supports model
averaging using an efficient Markov chain Monte Carlo (MCMC) algorithm.
This further optimizes the SBM models.

